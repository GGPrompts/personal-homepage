# Prompts Playground

A/B testing environment for comparing AI agents and prompts side by side.

## Files
- `app/sections/prompts-playground.tsx` - Main component
- `lib/prompts-playground.ts` - Types and localStorage utilities
- `components/prompts/QuadSplitViewer.tsx` - Resizable panel grid
- `components/prompts/BrowserPanel.tsx` - Individual iframe panel
- `components/prompts/ComparisonToolbar.tsx` - Actions toolbar
- `components/prompts/ComponentLibrary.tsx` - Saved components browser
- `components/prompts/SaveComponentDialog.tsx` - Save dialog
- `components/prompts/ComponentCard.tsx` - Library item display

## Features
- **Multi-Panel Comparison**:
  - 2x2 quad grid (default)
  - Horizontal split (2 side by side)
  - Vertical split (2 stacked)
  - Fullscreen single panel
  - Resizable panels with drag handles
- **Panel Configuration**:
  - Custom URL per panel (e.g., localhost:3001-3004)
  - Custom labels (Agent 1, Agent 2, etc.)
  - Agent config: CLI (claude/codex/gemini), model, system prompt
  - Individual refresh and save actions
- **Component Library**:
  - Save successful outputs with metadata
  - Attach files/code generated by agent
  - Tag-based organization
  - Search by name, prompt, or tags
  - Copy files to clipboard
  - Re-run prompts from saved components
- **Comparison Management**:
  - Save entire panel setups as named comparisons
  - Load saved comparisons instantly
  - Refresh all panels simultaneously
  - Screenshot all panels (placeholder)
- **Prompt Tracking**:
  - Reference prompt input for saving context
  - Prompts saved with components for reproducibility

## Workflow

1. **Setup Panels**: Configure each panel with a URL pointing to different agent dev servers
2. **Enter Prompt**: Type your test prompt in the reference input
3. **Run Tests**: Each panel shows its agent's output for the same prompt
4. **Compare**: Use view modes to compare outputs side by side
5. **Save Winners**: Save the best outputs to the Component Library with tags
6. **Save Setups**: Save the entire comparison setup for future A/B tests

## Agent Configuration
Each panel can be configured with:
- **CLI**: `claude`, `codex`, or `gemini`
- **Model**: Specific model ID (e.g., `claude-3-opus`, `gpt-4o`)
- **System Prompt**: Custom instructions for the agent
- **Agent**: Named agent configuration
- **Flags**: Additional CLI flags

## TabzChrome Selectors
- `data-tabz-section="prompts-playground"` - Container
- `data-tabz-input="current-prompt"` - Prompt input
- `data-tabz-region="quad-viewer"` - 2x2 grid view
- `data-tabz-region="horizontal-viewer"` - Side by side view
- `data-tabz-region="vertical-viewer"` - Stacked view
- `data-tabz-region="comparison-toolbar"` - Bottom toolbar
- `data-tabz-action="refresh-all"` - Refresh all panels
- `data-tabz-action="screenshot-all"` - Screenshot all panels
- `data-tabz-action="open-library"` - Open component library

## State
- `prompts-playground-panels` - Panel configurations
- `prompts-playground-components` - Saved components with files/prompts
- `prompts-playground-comparisons` - Saved comparison setups

All state persisted to localStorage.

## Sub-Items
- `library` - Opens the Component Library sheet

## Use Cases
- Compare different AI models on the same task
- Test system prompt variations
- Evaluate agent configurations
- Build a library of high-quality AI outputs
- Document which prompts work best for specific tasks
